policy_ensemble_test = {
    "exp_label": "Ensemble-Ising20-001",
    "task": "AltIsing20-v0",
    "use_ensemble_learner": True,
    "max_num_queries": 8000, # Maximum number of queries in experiment
    "num_inner_updates": 1,
    "num_policies": 4, # Essentially, the number of policies -- and this is equivalent to the num_query_proxies in MetaRLBO
    "num_initial_samples": 50, # Number of random samples that we intiially begin with
    "num_samples_per_policy": 10, # The number of samples generated by each of the policies
    "num_query_per_iter": 10, # Number of queries per round
    "num_updates_per_iter": 1,  # Number of updates per iter for each of the policies! 
    "entropy_reg_coeff": 0.2,
    "proxy_oracle": {
        "model_name": "CNN",
        "p": 1.0, 
    },
    # "outerloop": { # This is for meta-learning
    #     "oracle": "proxy",
    #     "density_penalty": True,
    # },
    "policy_lr": 1.0,
    "optimizer": "PPO", # Either PPO or REINFORCE -- REINFORCE is currently not working yet
    "ppo_config": { # Leo: This should be merged into train_policy_config --
        "clip_param": 0.2,
        "ppo_epoch": 4,
        "num_mini_batch": 4,
        "value_loss_coef": 0.5,
        "entropy_coef": 0.01,
        "lr": 7e-4,
        "eps": 1e-5,
        "max_grad_norm": 0.5,
        "use_gae": True,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "use_proper_time_limits": False,
        # "num_steps": 10,
    },
    "selection_criteria": { # Configs for selecting the samples
        "method": "UCB", 
        "config": {
            'beta': 1.0,
        },
        "diversity_threshold": 1, # Diversity threshold when greedily selecting molecules...
    },
    "env": { # See DynaPPO paper for these configs
        "lambda": 1.0, # Diversity hyperparameter -- higher is more penalty for more similar mols.
        "radius": 2, 
    },

    "true_oracle": {
        "model_name": "AltIsing_Oracle",
    },
    "log_interval": 1,
    "results_log_dir": "./logs",
    "seed": 73,
}

policy_ensemble_i20_001 = { # Based on Ising20_063
    "exp_label": "Policy_Ensemble-Ising20-001",
    "task": "AltIsing20-v0",
    "use_ensemble_learner": True,
    "max_num_queries": 8000, # Maximum number of queries in experiment
    "num_inner_updates": 1,
    "num_policies": 32, # Essentially, the number of policies -- and this is equivalent to the num_query_proxies in MetaRLBO
    "num_initial_samples": 500, # Number of random samples that we intiially begin with
    "num_samples_per_policy": 64, # The number of samples generated by each of the policies
    "num_query_per_iter": 500, # Number of queries per round
    "num_updates_per_iter": 80,  # Number of updates per iter for each of the policies! 
    "entropy_reg_coeff": 0.2,
    "proxy_oracle": {
        "model_name": "CNN",
        "p": 1.0, 
    },
    # "outerloop": { # This is for meta-learning
    #     "oracle": "proxy",
    #     "density_penalty": True,
    # },
    # "policy_lr": 1.0,
    "optimizer": "PPO", # Either PPO or REINFORCE -- REINFORCE is currently not working yet
    "ppo_config": { # Leo: This should be merged into train_policy_config --
        "clip_param": 0.2,
        "ppo_epoch": 4,
        "num_mini_batch": 4,
        "value_loss_coef": 0.5,
        "entropy_coef": 0.01,
        "lr": 7e-4,
        "eps": 1e-5,
        "max_grad_norm": 0.5,
        "use_gae": True,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "use_proper_time_limits": False,
        # "num_steps": 10,
    },
    "selection_criteria": { # Configs for selecting the samples
        "method": "UCB", 
        "config": {
            'beta': 1.0,
        },
        "diversity_threshold": 1, # Diversity threshold when greedily selecting molecules...
    },
    "env": { # See DynaPPO paper for these configs
        "lambda": 1.0, # Diversity hyperparameter -- higher is more penalty for more similar mols.
        "radius": 2, 
    },

    "true_oracle": {
        "model_name": "AltIsing_Oracle",
    },
    "reset_policy_per_round": False,
    "log_interval": 1,
    "results_log_dir": "./logs",
    "seed": 73,
}

policy_ensemble_i20_002 = { # Copy 001: but reset policy per round!
    "exp_label": "Policy_Ensemble-Ising20-002",
    "task": "AltIsing20-v0",
	"use_ensemble_learner": True,
    "max_num_queries": 8000, # Maximum number of queries in experiment
    "num_inner_updates": 1,
    "num_policies": 32, # Essentially, the number of policies -- and this is equivalent to the num_query_proxies in MetaRLBO
    "num_initial_samples": 500, # Number of random samples that we intiially begin with
    "num_samples_per_policy": 64, # The number of samples generated by each of the policies
    "num_query_per_iter": 500, # Number of queries per round
    "num_updates_per_iter": 80,  # Number of updates per iter for each of the policies! 
    "entropy_reg_coeff": 0.2,
    "proxy_oracle": {
        "model_name": "CNN",
        "p": 1.0, 
    },
    # "outerloop": { # This is for meta-learning
    #     "oracle": "proxy",
    #     "density_penalty": True,
    # },
    # "policy_lr": 1.0,
    "optimizer": "PPO", # Either PPO or REINFORCE -- REINFORCE is currently not working yet
    "ppo_config": { # Leo: This should be merged into train_policy_config --
        "clip_param": 0.2,
        "ppo_epoch": 4,
        "num_mini_batch": 4,
        "value_loss_coef": 0.5,
        "entropy_coef": 0.01,
        "lr": 7e-4,
        "eps": 1e-5,
        "max_grad_norm": 0.5,
        "use_gae": True,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "use_proper_time_limits": False,
        # "num_steps": 10,
    },
    "selection_criteria": { # Configs for selecting the samples
        "method": "UCB", 
        "config": {
            'beta': 1.0,
        },
        "diversity_threshold": 1, # Diversity threshold when greedily selecting molecules...
    },
    "env": { # See DynaPPO paper for these configs
        "lambda": 1.0, # Diversity hyperparameter -- higher is more penalty for more similar mols.
        "radius": 2, 
    },

    "true_oracle": {
        "model_name": "AltIsing_Oracle",
    },
    "reset_policy_per_round": True,
    "log_interval": 1,
    "results_log_dir": "./logs",
    "seed": 73,
}


rna14_026_qp_8 = { # 023 but beta: 1 -> 0
    "exp_label": "MetaRLBO-RNA14-026-QP-8",
    "task": "RNA14-v0",
    "num_proxies": 4, 
    "max_num_queries": 1500, # Maximum number of queries in experiment
    "num_inner_updates": 2,
    "num_query_proxies": 8,
    "num_initial_samples": 100,
    "num_samples_per_proxy": 256,
    "num_query_per_iter": 100,
    "inner_lr": 1.0,
    "outer_lr": 0.1,
    "num_meta_updates_per_iter": 50, 
    "entropy_reg_coeff": 0.2,
    "proxy_oracle": {
        "model_name": "CNN",
        "p": 1.0, 
    },
    "policy": {
        "num_steps": 58, # number of steps (per env) before updating... ensure this is at least as big as the length of the episode of the environment
        "num_meta_steps": 58,
    },
    "outerloop": {
        "oracle": "proxy",
        "density_penalty": True,
    },
    "selection_criteria": { # Configs for selecting the samples
        "method": "UCB", 
        "config": {
            'beta': 0.0,
        },
        "diversity_threshold": 1, # Diversity threshold when greedily selecting molecules...
    },
    "env": { # See DynaPPO paper for these configs
        "lambda": 0.1, # Diversity hyperparameter -- higher is more penalty for more similar mols.
        "radius": 2, 
    },

    "true_oracle": {
        "model_name": "RNA14_Oracle",
    },
    "reset_policy_per_round": True,
    "use_baseline": False,
    "log_interval": 1,
    "results_log_dir": "./logs",
    "seed": 73,
}




policy_ensemble_rna_001= { # Based on RNA14_026
    "exp_label": "Policy_Ensemble-RNA14-001",
    "task": "RNA14-v0",
    "use_ensemble_learner": True,
    "max_num_queries": 1500, # Maximum number of queries in experiment
    "num_policies": 8, # Essentially, the number of policies -- and this is equivalent to the num_query_proxies in MetaRLBO
    "num_initial_samples": 100,
    "num_samples_per_policy": 256,
    "num_query_per_iter": 100,
    "num_updates_per_iter": 50,  # Number of updates per iter for each of the policies! 
    "entropy_reg_coeff": 0.2,
    "proxy_oracle": {
        "model_name": "CNN",
        "p": 1.0, 
    },
    "policy": {
        "num_steps": 58, # number of steps (per env) before updating... ensure this is at least as big as the length of the episode of the environment
        "num_meta_steps": 58,
    },
    # "outerloop": { # This is for meta-learning
    #     "oracle": "proxy",
    #     "density_penalty": True,
    # },
    # "policy_lr": 1.0,
    "optimizer": "PPO", # Either PPO or REINFORCE -- REINFORCE is currently not working yet
    "ppo_config": { # Leo: This should be merged into train_policy_config --
        "clip_param": 0.2,
        "ppo_epoch": 4,
        "num_mini_batch": 4,
        "value_loss_coef": 0.5,
        "entropy_coef": 0.01,
        "lr": 7e-4,
        "eps": 1e-5,
        "max_grad_norm": 0.5,
        "use_gae": True,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "use_proper_time_limits": False,
        # "num_steps": 10,
    },
    "selection_criteria": { # Configs for selecting the samples
        "method": "UCB", 
        "config": {
            'beta': 0.0,
        },
        "diversity_threshold": 1, # Diversity threshold when greedily selecting molecules...
    },
    "env": { # See DynaPPO paper for these configs
        "lambda": 0.1, # Diversity hyperparameter -- higher is more penalty for more similar mols.
        "radius": 2, 
    },

    "true_oracle": {
        "model_name": "RNA14_Oracle",
    },
    "reset_policy_per_round": True,
    "log_interval": 1,
    "results_log_dir": "./logs",
    "seed": 73,
}


policy_ensemble_rna_002= { # Based on RNA14_026
    "exp_label": "Policy_Ensemble-RNA14-002",
    "task": "RNA14-v0",
    "use_ensemble_learner": True,
    "max_num_queries": 1500, # Maximum number of queries in experiment
    "num_policies": 4, # Essentially, the number of policies -- and this is equivalent to the num_query_proxies in MetaRLBO
    "num_initial_samples": 100,
    "num_samples_per_policy": 512,
    "num_query_per_iter": 100,
    "num_updates_per_iter": 50,  # Number of updates per iter for each of the policies! 
    "entropy_reg_coeff": 0.2,
    "proxy_oracle": {
        "model_name": "CNN",
        "p": 1.0, 
    },
    "policy": {
        "num_steps": 58, # number of steps (per env) before updating... ensure this is at least as big as the length of the episode of the environment
        "num_meta_steps": 58,
    },
    # "outerloop": { # This is for meta-learning
    #     "oracle": "proxy",
    #     "density_penalty": True,
    # },
    # "policy_lr": 1.0,
    "optimizer": "PPO", # Either PPO or REINFORCE -- REINFORCE is currently not working yet
    "ppo_config": { # Leo: This should be merged into train_policy_config --
        "clip_param": 0.2,
        "ppo_epoch": 4,
        "num_mini_batch": 4,
        "value_loss_coef": 0.5,
        "entropy_coef": 0.01,
        "lr": 7e-4,
        "eps": 1e-5,
        "max_grad_norm": 0.5,
        "use_gae": True,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "use_proper_time_limits": False,
        # "num_steps": 10,
    },
    "selection_criteria": { # Configs for selecting the samples
        "method": "UCB", 
        "config": {
            'beta': 0.0,
        },
        "diversity_threshold": 1, # Diversity threshold when greedily selecting molecules...
    },
    "env": { # See DynaPPO paper for these configs
        "lambda": 0.1, # Diversity hyperparameter -- higher is more penalty for more similar mols.
        "radius": 2, 
    },

    "true_oracle": {
        "model_name": "RNA14_Oracle",
    },
    "reset_policy_per_round": True,
    "log_interval": 1,
    "results_log_dir": "./logs",
    "seed": 73,
}

